{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8bf8532-584b-4578-a8f4-6c5ba0bc044d",
   "metadata": {},
   "source": [
    "# Tree based models for classification and regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff83a217-bc2a-4c73-87fc-5b22898bd2ab",
   "metadata": {},
   "source": [
    "# Classification Tree\n",
    "* given a labeled data set a classification learns a sequence of if-else questions about individual features in order to infer labels\n",
    "* in contrast to linear models, trees are able to capture nonlinear relationships between features and labels\n",
    "* trees don't require the features to be scaled\n",
    "* The total number of branches traversing the top to the bottom of the tree is known as the maximum depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfe7a85-0782-4571-b7a9-09aa73142732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c2cd8d-a812-4f2f-8392-4349ddc4a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = pd.read_csv('data/wisconsin_breast_cancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f39badaf-1f0b-4d46-9103-bd66cb8dac7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd507a6-cf5d-432f-84d8-e13055e5bfbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0677072c-a2fd-477c-a0a3-2d1d7448495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer[['radius_mean','concave points_mean']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c284cae0-dd2d-40a4-80f6-ab536fc6f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cancer['diagnosis']\n",
    "y = y.map(dict(M=1, B=0))\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2931caf6-12e5-4c14-b1c6-e97701256694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 2), (569,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "443d6f3b-2d6b-4891-a9bd-2b397d3d4e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9035087719298246"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#import train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import accuracy-score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#split data into 80/20 train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y, random_state=1)\n",
    "\n",
    "#instantiate decision tree classifier\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "#fit\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#test (predict)\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "#score the model\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796331e8-6517-42d4-a318-a243e3780166",
   "metadata": {},
   "source": [
    "### To understand the tree's predictions more concretely let's see how it classifies instances in feature-space\n",
    "* a classification model divides the feature space into regions where all instances are assigned to only one class-label\n",
    "* these regions are decision regions\n",
    "    * separated by spaces known as decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8933920-1ffb-4201-b1b3-47ac0a479d93",
   "metadata": {},
   "source": [
    "# <font color='red'> **** GIVEN FUNCTION **** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23636dd2-0b0d-4517-b4da-a8ba608bd433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_labeled_decision_regions(X,y, models):    \n",
    "    '''\n",
    "    Function producing a scatter plot of the instances contained \n",
    "    in the 2D dataset (X,y) along with the decision \n",
    "    regions of two trained classification models contained in the\n",
    "    list 'models'.\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pandas DataFrame corresponding to two numerical features \n",
    "    y: pandas Series corresponding the class labels\n",
    "    models: list containing two trained classifiers \n",
    "    \n",
    "    '''\n",
    "    if len(models) != 2:\n",
    "        raise Exception('''\n",
    "        Models should be a list containing only two trained classifiers.\n",
    "        ''')\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        raise Exception('''\n",
    "        X has to be a pandas DataFrame with two numerical features.\n",
    "        ''')\n",
    "    if not isinstance(y, pd.Series):\n",
    "        raise Exception('''\n",
    "        y has to be a pandas Series corresponding to the labels.\n",
    "        ''')\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(6.0,2.7), sharey=True)\n",
    "    for i, model in enumerate(models):\n",
    "        plot_decision_regions(X.values,y.values, model, legend= 2, ax = ax[i])\n",
    "        ax[i].set_title(model.__class__.__name__)\n",
    "        ax[i].set_xlabel(X.columns[0])\n",
    "        if i == 0:\n",
    "            ax[i].set_ylabel(X.columns[1])\n",
    "        ax[i].set_ylim(X.values[:,1].min(), X.values[:,1].max())\n",
    "        ax[i].set_xlim(X.values[:,0].min(), X.values[:,0].max())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_decision_regions(X, y, clf,\n",
    "                          feature_index=None,\n",
    "                          filler_feature_values=None,\n",
    "                          filler_feature_ranges=None,\n",
    "                          ax=None,\n",
    "                          X_highlight=None,\n",
    "                          res=0.02, legend=1,\n",
    "                          hide_spines=True,\n",
    "                          markers='s^oxv<>',\n",
    "                          colors='red,blue,limegreen,gray,cyan'):\n",
    "    \"\"\"Plot decision regions of a classifier.\n",
    "\n",
    "    Please note that this functions assumes that class labels are\n",
    "    labeled consecutively, e.g,. 0, 1, 2, 3, 4, and 5. If you have class\n",
    "    labels with integer labels > 4, you may want to provide additional colors\n",
    "    and/or markers as `colors` and `markers` arguments.\n",
    "    See http://matplotlib.org/examples/color/named_colors.html for more\n",
    "    information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape = [n_samples, n_features]\n",
    "        Feature Matrix.\n",
    "    y : array-like, shape = [n_samples]\n",
    "        True class labels.\n",
    "    clf : Classifier object.\n",
    "        Must have a .predict method.\n",
    "    feature_index : array-like (default: (0,) for 1D, (0, 1) otherwise)\n",
    "        Feature indices to use for plotting. The first index in\n",
    "        `feature_index` will be on the x-axis, the second index will be\n",
    "        on the y-axis.\n",
    "    filler_feature_values : dict (default: None)\n",
    "        Only needed for number features > 2. Dictionary of feature\n",
    "        index-value pairs for the features not being plotted.\n",
    "    filler_feature_ranges : dict (default: None)\n",
    "        Only needed for number features > 2. Dictionary of feature\n",
    "        index-value pairs for the features not being plotted. Will use the\n",
    "        ranges provided to select training samples for plotting.\n",
    "    ax : matplotlib.axes.Axes (default: None)\n",
    "        An existing matplotlib Axes. Creates\n",
    "        one if ax=None.\n",
    "    X_highlight : array-like, shape = [n_samples, n_features] (default: None)\n",
    "        An array with data points that are used to highlight samples in `X`.\n",
    "    res : float or array-like, shape = (2,) (default: 0.02)\n",
    "        Grid width. If float, same resolution is used for both the x- and\n",
    "        y-axis. If array-like, the first item is used on the x-axis, the\n",
    "        second is used on the y-axis. Lower values increase the resolution but\n",
    "        slow down the plotting.\n",
    "    hide_spines : bool (default: True)\n",
    "        Hide axis spines if True.\n",
    "    legend : int (default: 1)\n",
    "        Integer to specify the legend location.\n",
    "        No legend if legend is 0.\n",
    "    markers : str (default 's^oxv<>')\n",
    "        Scatterplot markers.\n",
    "    colors : str (default 'red,blue,limegreen,gray,cyan')\n",
    "        Comma separated list of colors.\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    ax : matplotlib.axes.Axes object\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    check_Xy(X, y, y_int=True)  # Validate X and y arrays\n",
    "    dim = X.shape[1]\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if isinstance(res, float):\n",
    "        xres, yres = res, res\n",
    "    else:\n",
    "        try:\n",
    "            xres, yres = res\n",
    "        except ValueError:\n",
    "            raise ValueError('Unable to unpack res. Expecting '\n",
    "                             'array-like input of length 2.')\n",
    "\n",
    "    plot_testdata = True\n",
    "    if not isinstance(X_highlight, np.ndarray):\n",
    "        if X_highlight is not None:\n",
    "            raise ValueError('X_highlight must be a NumPy array or None')\n",
    "        else:\n",
    "            plot_testdata = False\n",
    "    elif len(X_highlight.shape) < 2:\n",
    "        raise ValueError('X_highlight must be a 2D array')\n",
    "\n",
    "    if feature_index is not None:\n",
    "        # Unpack and validate the feature_index values\n",
    "        if dim == 1:\n",
    "            raise ValueError(\n",
    "                'feature_index requires more than one training feature')\n",
    "        try:\n",
    "            x_index, y_index = feature_index\n",
    "        except ValueError:\n",
    "            raise ValueError(\n",
    "                'Unable to unpack feature_index. Make sure feature_index '\n",
    "                'only has two dimensions.')\n",
    "        try:\n",
    "            X[:, x_index], X[:, y_index]\n",
    "        except IndexError:\n",
    "            raise IndexError(\n",
    "                'feature_index values out of range. X.shape is {}, but '\n",
    "                'feature_index is {}'.format(X.shape, feature_index))\n",
    "    else:\n",
    "        feature_index = (0, 1)\n",
    "        x_index, y_index = feature_index\n",
    "\n",
    "    # Extra input validation for higher number of training features\n",
    "    if dim > 2:\n",
    "        if filler_feature_values is None:\n",
    "            raise ValueError('Filler values must be provided when '\n",
    "                             'X has more than 2 training features.')\n",
    "\n",
    "        if filler_feature_ranges is not None:\n",
    "            if not set(filler_feature_values) == set(filler_feature_ranges):\n",
    "                raise ValueError(\n",
    "                    'filler_feature_values and filler_feature_ranges must '\n",
    "                    'have the same keys')\n",
    "\n",
    "        # Check that all columns in X are accounted for\n",
    "        column_check = np.zeros(dim, dtype=bool)\n",
    "        for idx in filler_feature_values:\n",
    "            column_check[idx] = True\n",
    "        for idx in feature_index:\n",
    "            column_check[idx] = True\n",
    "        if not all(column_check):\n",
    "            missing_cols = np.argwhere(~column_check).flatten()\n",
    "            raise ValueError(\n",
    "                'Column(s) {} need to be accounted for in either '\n",
    "                'feature_index or filler_feature_values'.format(missing_cols))\n",
    "\n",
    "    marker_gen = cycle(list(markers))\n",
    "\n",
    "    n_classes = np.unique(y).shape[0]\n",
    "    colors = colors.split(',')\n",
    "    colors_gen = cycle(colors)\n",
    "    colors = [next(colors_gen) for c in range(n_classes)]\n",
    "\n",
    "    # Get minimum and maximum\n",
    "    x_min, x_max = X[:, x_index].min() - 1, X[:, x_index].max() + 1\n",
    "    if dim == 1:\n",
    "        y_min, y_max = -1, 1\n",
    "    else:\n",
    "        y_min, y_max = X[:, y_index].min() - 1, X[:, y_index].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, xres),\n",
    "                         np.arange(y_min, y_max, yres))\n",
    "\n",
    "    if dim == 1:\n",
    "        X_predict = np.array([xx.ravel()]).T\n",
    "    else:\n",
    "        X_grid = np.array([xx.ravel(), yy.ravel()]).T\n",
    "        X_predict = np.zeros((X_grid.shape[0], dim))\n",
    "        X_predict[:, x_index] = X_grid[:, 0]\n",
    "        X_predict[:, y_index] = X_grid[:, 1]\n",
    "        if dim > 2:\n",
    "            for feature_idx in filler_feature_values:\n",
    "                X_predict[:, feature_idx] = filler_feature_values[feature_idx]\n",
    "    Z = clf.predict(X_predict)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot decisoin region\n",
    "    ax.contourf(xx, yy, Z,\n",
    "                alpha=0.3,\n",
    "                colors=colors,\n",
    "                levels=np.arange(Z.max() + 2) - 0.5)\n",
    "\n",
    "    ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n",
    "\n",
    "    # Scatter training data samples\n",
    "    for idx, c in enumerate(np.unique(y)):\n",
    "        if dim == 1:\n",
    "            y_data = [0 for i in X[y == c]]\n",
    "            x_data = X[y == c]\n",
    "        elif dim == 2:\n",
    "            y_data = X[y == c, y_index]\n",
    "            x_data = X[y == c, x_index]\n",
    "        elif dim > 2 and filler_feature_ranges is not None:\n",
    "            class_mask = y == c\n",
    "            feature_range_mask = get_feature_range_mask(\n",
    "                            X, filler_feature_values=filler_feature_values,\n",
    "                            filler_feature_ranges=filler_feature_ranges)\n",
    "            y_data = X[class_mask & feature_range_mask, y_index]\n",
    "            x_data = X[class_mask & feature_range_mask, x_index]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        ax.scatter(x=x_data,\n",
    "                   y=y_data,\n",
    "                   alpha=0.8,\n",
    "                   c=colors[idx],\n",
    "                   marker=next(marker_gen),\n",
    "                   edgecolor='black',\n",
    "                   label=c)\n",
    "\n",
    "    if hide_spines:\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    if dim == 1:\n",
    "        ax.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "    if legend:\n",
    "        if dim > 2 and filler_feature_ranges is None:\n",
    "            pass\n",
    "        else:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            ax.legend(handles, labels,\n",
    "                      framealpha=0.3, scatterpoints=1, loc=legend)\n",
    "\n",
    "    if plot_testdata:\n",
    "        if dim == 1:\n",
    "            x_data = X_highlight\n",
    "            y_data = [0 for i in X_highlight]\n",
    "        elif dim == 2:\n",
    "            x_data = X_highlight[:, x_index]\n",
    "            y_data = X_highlight[:, y_index]\n",
    "        else:\n",
    "            feature_range_mask = get_feature_range_mask(\n",
    "                    X_highlight, filler_feature_values=filler_feature_values,\n",
    "                    filler_feature_ranges=filler_feature_ranges)\n",
    "            y_data = X_highlight[feature_range_mask, y_index]\n",
    "            x_data = X_highlight[feature_range_mask, x_index]\n",
    "\n",
    "        ax.scatter(x_data,\n",
    "                   y_data,\n",
    "                   c='',\n",
    "                   edgecolor='black',\n",
    "                   alpha=1.0,\n",
    "                   linewidths=1,\n",
    "                   marker='o',\n",
    "                   s=80)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587af654-362b-4a1e-acaa-b416f825547f",
   "metadata": {},
   "source": [
    "# Logistic regression vs classification tree\n",
    "\n",
    "A classification tree divides the feature space into rectangular regions. In contrast, a linear model such as logistic regression produces only a single linear decision boundary dividing the feature space into two decision regions.\n",
    "\n",
    "We have written a custom function called plot_labeled_decision_regions() that you can use to plot the decision regions of a list containing two trained classifiers. You can type help(plot_labeled_decision_regions) in the IPython shell to learn more about this function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb3db9-a18e-4262-866d-63543095f9a6",
   "metadata": {},
   "source": [
    "# <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a9e974-67da-4007-8bbc-7a3547746e80",
   "metadata": {},
   "source": [
    "# Classification Tree Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f63bd-b8c5-4c10-804b-c08c7f0a16b4",
   "metadata": {},
   "source": [
    "* A decision tree is a data structure consisting of a hierarchy of individual units called nodes\n",
    "* Node\n",
    "    * question or prediction\n",
    "* the root is where the decision begins to grow\n",
    "    * it has no parent node\n",
    "* internal node has a parent\n",
    "    * question that gives rise to two more children nodes\n",
    "* Leaf\n",
    "    * no children\n",
    "    * prediction is made here\n",
    "* At each node the model asks a question\n",
    "    * it is trying to create the purest leafs possible\n",
    "    * so how does the model know what questions to ask and features to look at?\n",
    "    * that is, how does it know how to split?\n",
    "* It maximizes information gain\n",
    "    * it knows each node contains information \n",
    "* Criterions used to measure impurity of a node\n",
    "    * Entropy\n",
    "    * GINI index\n",
    "* Summary\n",
    "    * Nodes grow recursively\n",
    "        * a node exists based on the state of its predecessors\n",
    "        * feature f and split point sp to maximize information gain\n",
    "* if the informatino gain at the node = 0, the node is declared as a leaf\n",
    "    * these rules are for unconstrained trees\n",
    "    * if we constrain the depth of the tree, to say 2, \n",
    "        * all nodes with depth of 2 will be declared as leaves even if the information gain is > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3681d37-3f1b-48d6-99f8-ead500961367",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#import train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import accuracy-score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#split data into 80/20 train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y, random_state=1)\n",
    "\n",
    "#instantiate decision tree classifier with criterion='gini' (no constraints)\n",
    "dt = DecisionTreeClassifier(criterion='gini', random_state=1)\n",
    "\n",
    "#fit\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#test (predict)\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "#score the model\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0dbc6-c550-4ba3-b59a-4d656ee432ba",
   "metadata": {},
   "source": [
    "### We have a higher accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006602ff-5680-48da-a1e2-3f8302af9309",
   "metadata": {},
   "source": [
    "# <hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eebb97-8f69-4a7b-bf10-dc1c04419e5b",
   "metadata": {},
   "source": [
    "# Decision Tree for Regression\n",
    "* Target is continuous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd1e0d6c-cebc-420f-a414-59da5e496216",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = pd.read_csv('data/mpg.csv')\n",
    "\n",
    "mpg.head(1)\n",
    "\n",
    "y = mpg['mpg'].values\n",
    "y.shape\n",
    "\n",
    "# for simplicity we begin only 1 feature, namely displacement\n",
    "X = mpg['displ'].values\n",
    "X.shape\n",
    "\n",
    "y.reshape(-1,1)\n",
    "\n",
    "X.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3f8ec4b-6bca-4769-8e0a-e56b3625066f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function ndarray.reshape>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59b65725-ecbd-4143-ab36-65c2c390bbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.scatterplot(x=X, y=y, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d1256-7bd9-484e-8e5e-66ad5ed6e2c0",
   "metadata": {},
   "source": [
    "### we see that the relationship between displacement and mpg is nonlinear\n",
    "* nonlinear models will not do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37e61369-be33-4359-9c02-d0ce42d31906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.102306888903137\n"
     ]
    }
   ],
   "source": [
    "#import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, 1),y, test_size=0.2, random_state=3)\n",
    "\n",
    "#instantiate Decision tree regressor\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.1, random_state=3)\n",
    "\n",
    "# fit\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#predict\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "\n",
    "#get root mean squared errot\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "#compute test-set RMSE by raising to power 1/2\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "#print MSE\n",
    "print(rmse_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf984f-de79-4c8d-9da5-0c80223a88d3",
   "metadata": {},
   "source": [
    "# <hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20559689-f936-46ea-bf24-84874a120cf5",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "# Generalization Error\n",
    "### We assume that there is a mapping y = f(x)\n",
    "* Goals of supervised learning\n",
    "    * find a model f_hat that best approximates f: f_hat ~ f\n",
    "    * f_hat can be logistic, decision tree, neural network...\n",
    "    * discard as much as possible\n",
    "    * end goal: f_hat should achieve a low predictive error on unseen datasets\n",
    "* difficulties when approximating f_hat\n",
    "    * overfitting\n",
    "        * fits the noise on the training set\n",
    "        * prediction capabilities are low \n",
    "        * it memorizes noise and causes high test set error\n",
    "    * underfitting\n",
    "        * not flexible enough to fit the data\n",
    "        * training set error is almost equal to test set error\n",
    "        * both errors will be high\n",
    "        * \"like teaching calculus to a 3 year old. it does not have the mental abstraction to extrapolate to new things.\"\n",
    "        * high bias\n",
    "* generalization error tells you how much it generalizes on unseen data\n",
    "    * f_hat = bias^2 + variance + irreducible erro\n",
    "    * bias\n",
    "        * how much is f_hat not equal to f\n",
    "    * variance\n",
    "        * how much f_hat is inconsistent over different training sets\n",
    "        \n",
    "* Model complexity\n",
    "    * sets the flexibility of f_hat\n",
    "    * ex: max tree depth, min samples per leaf, ...\n",
    "* Bias-variance tradeoff\n",
    "    * when model complexity increases, the variance increases while the bias decreases\n",
    "    * when complexity decreases, varaince decreases and bias increases\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc10c3-b516-4c02-b298-1f79d4e4d0f8",
   "metadata": {},
   "source": [
    "# <hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9b923-0a85-47a3-8aeb-2c53f7d747c9",
   "metadata": {},
   "source": [
    "# Diganosing Bias and Variance Problems\n",
    "* how do we estimate the generalization error of a model?\n",
    "* cannot be done directly because f is unknown\n",
    "* split into test/train set\n",
    "* fit f_hat to training set\n",
    "* evaluate the error of f_hat on the unseen test set\n",
    "* generalization error of f_hat ~ test set error of f_hat\n",
    "    * We use cross validation! \n",
    "        * k-fold validation\n",
    "    * get f_hat error on training set\n",
    "    * get f_hat error on test set\n",
    "    * compare the 2 errors\n",
    "    * if cv_error_f_hat > training_set_error_f_hat\n",
    "        * f_hat has high variance and is overfit\n",
    "        * decrease f_hat complexity\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f996ad2-acd8-4ba7-ac23-9f25a90bfcbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "mpg = pd.read_csv('data/mpg.csv')\n",
    "\n",
    "mpg.head(1)\n",
    "\n",
    "y = mpg['mpg'].values\n",
    "y.shape\n",
    "\n",
    "# for simplicity we begin only 1 feature, namely displacement\n",
    "X = mpg['displ'].values\n",
    "X.shape, y.shape\n",
    "\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "X = X.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a4a6efeb-4cda-43b8-9c8b-8ae1445d5482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MSE: 19.94\n"
     ]
    }
   ],
   "source": [
    "SEED = 123\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.14, random_state=SEED)\n",
    "\n",
    "MSE_CV = -cross_val_score(dt, X_train, y_train, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "#result is numpy array of the 10 negative mean-squared-erros achieved on the 10 folds\n",
    "# you can multiply the array by -1\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_predict_train = dt.predict(X_train)\n",
    "\n",
    "y_predict_test = dt.predict(X_test)\n",
    "\n",
    "\n",
    "#CV MSE\n",
    "print('CV MSE: {:.2f}'.format(MSE_CV.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e40552e4-e93a-4705-8217-f69d5970fd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 17.89\n"
     ]
    }
   ],
   "source": [
    "# Training set MSE\n",
    "print(\"Train MSE: {:.2f}\".format(MSE(y_train, y_predict_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9644489-6091-4997-93c3-b23041f5d8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 20.41\n"
     ]
    }
   ],
   "source": [
    "#Test Set MSE\n",
    "print(\"Test MSE: {:.2f}\".format(MSE(y_test, y_predict_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02285449-63ea-437a-a29a-c11ae84b27fb",
   "metadata": {},
   "source": [
    "### Training set error is less than the CV error\n",
    "* we can deduce that dt overfits the training set and that it suffers from high variance\n",
    "* test set and cv errors are roughly equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "759f438a-ee01-4e97-a8ed-a4a445fcbb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>displ</th>\n",
       "      <th>hp</th>\n",
       "      <th>weight</th>\n",
       "      <th>accel</th>\n",
       "      <th>origin</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>88</td>\n",
       "      <td>3139</td>\n",
       "      <td>14.5</td>\n",
       "      <td>US</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>193</td>\n",
       "      <td>4732</td>\n",
       "      <td>18.5</td>\n",
       "      <td>US</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>60</td>\n",
       "      <td>1800</td>\n",
       "      <td>16.4</td>\n",
       "      <td>Asia</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>98</td>\n",
       "      <td>3525</td>\n",
       "      <td>19.0</td>\n",
       "      <td>US</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.3</td>\n",
       "      <td>97.0</td>\n",
       "      <td>78</td>\n",
       "      <td>2188</td>\n",
       "      <td>15.8</td>\n",
       "      <td>Europe</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  displ   hp  weight  accel  origin  size\n",
       "0  18.0  250.0   88    3139   14.5      US  15.0\n",
       "1   9.0  304.0  193    4732   18.5      US  20.0\n",
       "2  36.1   91.0   60    1800   16.4    Asia  10.0\n",
       "3  18.5  250.0   98    3525   19.0      US  15.0\n",
       "4  34.3   97.0   78    2188   15.8  Europe  10.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc3b2f-31f9-4e5a-a857-ebe4e1fa4f3d",
   "metadata": {},
   "source": [
    "# Instantiate the model\n",
    "\n",
    "In the following set of exercises, you'll diagnose the bias and variance problems of a regression tree. The regression tree you'll define in this exercise will be used to predict the mpg consumption of cars from the auto dataset using all available features.\n",
    "\n",
    "We have already processed the data and loaded the features matrix X and the array y in your workspace. In addition, the DecisionTreeRegressor class was imported from sklearn.tree.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "94fa9c70-464e-4a3e-8774-899880b8be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mpg.drop(['mpg', 'origin'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a04ba1cf-489f-4e30-a97b-b4fa2be36b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mpg.mpg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8626fb8f-76a9-4b7c-8eda-e44ea4f5c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set SEED for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split the data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=SEED)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor dt\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a30ed6b0-55ac-40fd-9be5-a7f8b2bd813c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: 5.14\n"
     ]
    }
   ],
   "source": [
    "# Compute the array containing the 10-folds CV MSEs\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       n_jobs=-1)\n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bf26daa0-cffb-4e07-809a-813f161a17d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 5.15\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = dt.predict(X_train)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
    "\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a51b4-c25e-4a76-b665-466935714ede",
   "metadata": {},
   "source": [
    "# <hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3490cde-ecb4-4b14-84fa-60b3f269b7e8",
   "metadata": {},
   "source": [
    "# Ensemble learning\n",
    "* Train different models on the same dataset\n",
    "* let each model make predictions\n",
    "* meta-model: aggregate predictions of individual models \n",
    "* final prediction: more robust and less prone to errors than each individual model\n",
    "* best results when models are skillful in different ways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3563e42-cde6-4b34-9860-3b1bfa25b2df",
   "metadata": {},
   "source": [
    "## Voting Classifier (with binary classification task)\n",
    "* Using breast cancer data set\n",
    "* X = all features\n",
    "* y = diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "96d9e1b1-1368-4350-90c5-12eb16da4d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer = pd.read_csv('data/wisconsin_breast_cancer.csv')\n",
    "cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "754f28f0-5090-4b0c-b973-727340767290",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer.drop(['id','diagnosis', 'Unnamed: 32'], axis=1)\n",
    "y = cancer.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f069be66-f251-43b6-b3fb-50892b02cd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "537e3eb0-7f98-43ac-8fad-0a1dd70b20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = SEED)\n",
    "\n",
    "#instantiate models\n",
    "lr = LogisticRegression(max_iter=2350,random_state=SEED)\n",
    "knn = KNN()\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "#define list called classifier that contains tuples corresponding to the names (classifier_name, classifier)\n",
    "classifiers = [(\"Logistic Regression\", lr), ('K Nearest Neighbors', knn), ('Classification Tree', dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b78fb9fa-b668-487d-aa2c-cca2ea89a64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.947\n",
      "K Nearest Neighbors : 0.930\n",
      "Classification Tree : 0.930\n"
     ]
    }
   ],
   "source": [
    "# itereate over list of classifier to fit each classifier to the training set, evaluate its accuracy on the test set, and print the result\n",
    "for clf_name, clf in classifiers:\n",
    "    #fit clf to the training set\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    #predict labels of the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    #evaluate accuracy of clf on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "efcb180e-86a6-4057-b298-b9eb6a7dde58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting classifier: 0.9532163742690059\n"
     ]
    }
   ],
   "source": [
    "#instantiate voting classifier as vc\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "\n",
    "#fit 'vc' to the training set and predict the test labels\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# evaluate the test-set accuracy of 'vc'\n",
    "print('Voting classifier: {}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a14a3-fc02-4b7d-ad3b-47c3dfd373c6",
   "metadata": {},
   "source": [
    "# <hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a65b3-8158-437b-899f-4ba2720486e7",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "* Bootstrap aggregation a.k.a \"Bagging\"\n",
    "### Ensemble Methods\n",
    "* Voting classifier\n",
    "    * fit to the same training models\n",
    "    * not the same algorithm \n",
    "    * the final predictions are obtained by majority voting\n",
    "* Bagging\n",
    "    * ensemble is formed by one model that uses the same training algorithm\n",
    "    * not trained on the entire training set\n",
    "    * instead, each model is trained on a different subset of the data\n",
    "    * has the effect of reducing variance of individual models in the ensemble\n",
    "    * samples with replacement\n",
    "    * Training\n",
    "        * draws N different bootstrap samples from the training set\n",
    "        * each of the bootstrap samples are then used to train N-models that use the same algorithm\n",
    "        * when a new instance is fed to the different models forming the bagging ensemble, each model outputs its prediction\n",
    "        * The meta model collects these predictions and outputs a final prediction, depending on the nature of the problem\n",
    "    * Classification\n",
    "        * the output is produced by majority voting\n",
    "        * BaggingClassifier\n",
    "    * Regression\n",
    "        * the final prediction is the average of the predictions maded by the individual models forming the ensemble\n",
    "        * BaggingRegressor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "48dad880-1768-4939-bcb0-b5370499a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "cancer = pd.read_csv('data/wisconsin_breast_cancer.csv')\n",
    "cancer.head()\n",
    "\n",
    "X = cancer.drop(['id','diagnosis', 'Unnamed: 32'], axis=1)\n",
    "y = cancer.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0cff2d4a-5926-4bf5-8e60-7100d51bb3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bagging Classifier: 0.942\n"
     ]
    }
   ],
   "source": [
    "SEED=1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "#instantiate classification tree dt\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "#instantiate bagging classifier\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1)\n",
    "\n",
    "#fit bc to training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "#predict test labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "#evaluate and print test-set accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of Bagging Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f3cca-1d6a-4ca9-8ea7-d0d09ce5145e",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "# Define the bagging classifier\n",
    "\n",
    "In the following exercises you'll work with the Indian Liver Patient dataset from the UCI machine learning repository. Your task is to predict whether a patient suffers from a liver disease using 10 features including Albumin, age and gender. You'll do so using a Bagging Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e9de53f6-c0ee-4eca-9914-60d8f7862946",
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_liver = pd.read_csv('data/indian_liver_patient_preprocessed.csv')\n",
    "indian_liver.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "indian_liver.head(1)\n",
    "\n",
    "X = indian_liver.drop('Liver_disease', axis=1)\n",
    "y = indian_liver['Liver_disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5806d9b5-6a44-4789-b369-da5fabdc9ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cd0c7799-f542-492d-92b0-48e99b1af368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of bc: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Fit bc to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate acc_test\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "print('Test set accuracy of bc: {:.2f}'.format(acc_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a195531c-1cec-4c2e-8ca3-5d22e23d8408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "574    0\n",
       "575    1\n",
       "576    1\n",
       "577    1\n",
       "578    0\n",
       "Name: Liver_disease, Length: 579, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db357e72-1f79-463a-bdf2-7ca79d5d61e0",
   "metadata": {},
   "source": [
    "# <hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2556b588-d7a4-49c7-ab64-838d86a7c8d4",
   "metadata": {},
   "source": [
    "# Out of bag evaluation\n",
    "* some instances may be sampled several times for one model\n",
    "* other instances may not be sampled at all\n",
    "* On average, for each model 63% of the training instances are sampled\n",
    "* the remaining 37% constitute what is known as the out-of-bag instances (OOB)\n",
    "    * these are not seen by the model during training\n",
    "    * these can then be used to estimate the performance of the model without the need for cross validation\n",
    "    * known as OOB evaluation\n",
    "        * oob score \n",
    "            * accuracy for classifier\n",
    "            * r-squared score for regressors\n",
    "\n",
    "### How does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b9059755-2c29-4394-af68-6a788e09bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "cancer = pd.read_csv('data/wisconsin_breast_cancer.csv')\n",
    "cancer.head()\n",
    "\n",
    "X = cancer.drop(['id','diagnosis', 'Unnamed: 32'], axis=1)\n",
    "y = cancer.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cf80069b-328c-43eb-9efa-7a2956ef0514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.930\n",
      "OOB accuracy 0.917\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "#instantiate a bagging classifier, set oob_score=True\n",
    "# this allows us to evaluate the oob accuracy after training\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, oob_score=True, n_jobs=-1)\n",
    "\n",
    "#fit\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "#predict\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#evaluate oob accuracy from bc\n",
    "oob_accuracy = bc.oob_score_\n",
    "\n",
    "#print test set accuracy\n",
    "print(\"Test set accuracy: {:.3f}\".format(test_accuracy))\n",
    "\n",
    "#print oob accuracy\n",
    "print(\"OOB accuracy {:.3f}\".format(oob_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce42223-a88d-4f43-8d26-ff05c925a04a",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "# Prepare the ground\n",
    "\n",
    "In the following exercises, you'll compare the OOB accuracy to the test set accuracy of a bagging classifier trained on the Indian Liver Patient dataset.\n",
    "\n",
    "In sklearn, you can evaluate the OOB accuracy of an ensemble classifier by setting the parameter oob_score to True during instantiation. After training the classifier, the OOB accuracy can be obtained by accessing the .oob_score_ attribute from the corresponding instance.\n",
    "\n",
    "In your environment, we have made available the class DecisionTreeClassifier from sklearn.tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4119d36d-c1ed-4607-bfb4-c741f5491da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age_std</th>\n",
       "      <th>Total_Bilirubin_std</th>\n",
       "      <th>Direct_Bilirubin_std</th>\n",
       "      <th>Alkaline_Phosphotase_std</th>\n",
       "      <th>Alamine_Aminotransferase_std</th>\n",
       "      <th>Aspartate_Aminotransferase_std</th>\n",
       "      <th>Total_Protiens_std</th>\n",
       "      <th>Albumin_std</th>\n",
       "      <th>Albumin_and_Globulin_Ratio_std</th>\n",
       "      <th>Is_male_std</th>\n",
       "      <th>Liver_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.247403</td>\n",
       "      <td>-0.42032</td>\n",
       "      <td>-0.495414</td>\n",
       "      <td>-0.42887</td>\n",
       "      <td>-0.355832</td>\n",
       "      <td>-0.319111</td>\n",
       "      <td>0.293722</td>\n",
       "      <td>0.203446</td>\n",
       "      <td>-0.14739</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age_std  Total_Bilirubin_std  Direct_Bilirubin_std  \\\n",
       "0  1.247403             -0.42032             -0.495414   \n",
       "\n",
       "   Alkaline_Phosphotase_std  Alamine_Aminotransferase_std  \\\n",
       "0                  -0.42887                     -0.355832   \n",
       "\n",
       "   Aspartate_Aminotransferase_std  Total_Protiens_std  Albumin_std  \\\n",
       "0                       -0.319111            0.293722     0.203446   \n",
       "\n",
       "   Albumin_and_Globulin_Ratio_std  Is_male_std  Liver_disease  \n",
       "0                        -0.14739            0              1  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indian_liver = pd.read_csv('data/indian_liver_patient_preprocessed.csv')\n",
    "indian_liver.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "indian_liver.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e1ceb4e9-76d7-40e4-ab07-afb4952aad9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.942, OOB accuracy: 0.947\n"
     ]
    }
   ],
   "source": [
    "X = indian_liver.drop('Liver_disease', axis=1)\n",
    "y = indian_liver['Liver_disease']\n",
    "\n",
    "\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, \n",
    "            n_estimators=50,\n",
    "            oob_score=True,\n",
    "            random_state=1)\n",
    "\n",
    "# Fit bc to the training set \n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate OOB accuracy\n",
    "acc_oob = bc.oob_score_\n",
    "\n",
    "# Print acc_test and acc_oob\n",
    "print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb0d80-a778-41da-b667-f021b5ef54f5",
   "metadata": {},
   "source": [
    "# <hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f747350b-3f07-4ab4-b6d5-580c2944df05",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\"> Random Forests </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af8708c-db90-46b7-9d14-491e7e215748",
   "metadata": {},
   "source": [
    "# Another ensemble method\n",
    "* Base estimator = Decision Tree\n",
    "* each estimator is trained on a different bootstrap sample having the same size as the training set\n",
    "* introduces further randomization than bagging when training each of the base estimators\n",
    "* When each tree is trained, only d-features can be sampled at each node with out replacement\n",
    "    * (d < total number of features)\n",
    "* the node is then split using the sampled feature that maximizes information again\n",
    "* in scikit learn d defaults to the square root of the number of features\n",
    "* once trained, predictions can be made on new instances\n",
    "* when a new instance is fed to the different base estimators, each of them outputs a prediction\n",
    "    * the predictions are then collected by the random forests meta-classifier and the final prediction is made depending on the nature of the problem, i.e. classification//regression\n",
    "    * if classification: prediction is made by majority voting\n",
    "    * if regression: average of all lables predicted by the best estimators\n",
    "    * random forest achieves a lower variance than individual trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458439f-bd76-4078-a946-488d230156ae",
   "metadata": {},
   "source": [
    "### Auto dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0ad6ca89-66d5-4bc5-ab3d-1302523b3ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((392,), (392, 5))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "mpg = pd.read_csv('data/mpg.csv')\n",
    "\n",
    "mpg.head(1)\n",
    "\n",
    "y = mpg['mpg'].values\n",
    "y.shape\n",
    "\n",
    "# for simplicity we begin only 1 feature, namely displacement\n",
    "X = mpg.drop(['mpg','origin'], axis=1).values\n",
    "y = y.ravel()\n",
    "# X = X.reshape(-1,1)\n",
    "y.shape,X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "975508c8-7b13-4ca7-94eb-422941de68b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 3.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "SEED=1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.12, random_state=SEED)\n",
    "\n",
    "#fit\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305f13e-3c5b-40db-b67b-ba3d7b9d8377",
   "metadata": {},
   "source": [
    "### Access the feature importance from the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3f8aca86-a045-461a-b792-ed3e6bf6c6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "X = mpg.drop(['mpg','origin'], axis=1)\n",
    "y = mpg['mpg']\n",
    "#create a pd.series of features importances\n",
    "importances_rf = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "\n",
    "sorted_importances_rf.plot(kind='barh', color='lightgreen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17835fe-7970-4388-ad74-53aae13cd3a3",
   "metadata": {},
   "source": [
    "# <hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19699992-cb71-4bd6-8a9e-820f922ff80f",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\"> AdaBoost</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57332dbb-166a-4580-a72a-099e836c204f",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "* boosting referst to an ensemble method in which many predictors are trained and  each predictor learns from the errors of its predecessor\n",
    "* many weak learners are combined to form a strong learner\n",
    "* Weak Learner: model doing slightly better than random guessing... not well at all\n",
    "\n",
    "# Boosting:\n",
    "    * train an ensemble predictors sequentially\n",
    "    * each predictor tries to correct its predecessor\n",
    "* most common boosting methods\n",
    "    * AdaBoost\n",
    "    * Gradient Boosting\n",
    "* AdaBoost = Adaptive Boosting\n",
    "    * each predictor pays attention to the errors of its predecessor by constantly changing the weights of training instances\n",
    "    * each predictor is assigned a coefficient,  $ \\alpha\\ $\n",
    "    * $ \\alpha\\ $ depends on the predictor's training error\n",
    "    * there are n-predictors\n",
    "* Prediction\n",
    "    * classification\n",
    "        * weighted majority voting\n",
    "        * AdaBoostClassifier\n",
    "    * Regression\n",
    "        * weighted average\n",
    "        * AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8faf51e9-92c1-4ba6-bd50-3e271016ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = pd.read_csv('data/wisconsin_breast_cancer.csv')\n",
    "cancer.head()\n",
    "\n",
    "X = cancer.drop(['id','diagnosis', 'Unnamed: 32'], axis=1)\n",
    "y = cancer.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6d7b2a3d-e3c8-4817-9414-a88eee43c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED =1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, stratify=y, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ddacdd82-dcde-4214-ba22-b070968b89c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "fd744649-cd4f-4a4a-9bf7-1db0b51870f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3fc2212f-bfde-4ec2-95c6-da13bc46eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_clf.fit(X_train, y_train)\n",
    "\n",
    "#predict the test set probabilities of positive class\n",
    "#this enables you to evaluate ROC-AUC score of adb_clf\n",
    "y_pred_proba = adb_clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "01189ab0-c290-4209-a633-ec2a0578819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "dceeaafc-5013-439e-919c-fb615249090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.99\n"
     ]
    }
   ],
   "source": [
    "print('ROC AUC score: {:.2f}'.format(adb_clf_roc_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e1953f-c2c2-4665-884b-e95ac77cafd9",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\"> Gradient Boosting</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638b3b7-2b95-4710-8ccb-af2b9c425b28",
   "metadata": {},
   "source": [
    "* Sequential correction of predecessors errors\n",
    "* does not tweak weights of training instances\n",
    "* each predictor is trained using the residual errors of its predecessors as labels\n",
    "    * essentially, finding the residuals of residuals\n",
    "* Shrinkage\n",
    "    * the prediction of each tree in the ensemble is shrinked after it is multiplied by a learning rate $\\eta\\\\$\n",
    "        * 0 < $\\eta\\ $ < 1\n",
    "        \n",
    "    * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "86d79c9e-d245-4a70-89c2-56a05c9277f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mpg.drop(['mpg','origin'], axis=1)\n",
    "y = mpg['mpg']\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "526fc0f2-ae0d-4ee4-90c1-e3df97b2dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GradientBoostingRegressor(n_estimators=300, max_depth=1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7d93a01f-0e56-44fb-8806-5399159052a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.08300144003074\n"
     ]
    }
   ],
   "source": [
    "gbt.fit(X_train, y_train)\n",
    "y_pred = gbt.predict(X_test)\n",
    "\n",
    "#evaluate the test set for RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "print(rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de540880-5735-4b08-a98b-b1674b8e91a5",
   "metadata": {},
   "source": [
    "<h1 style='text-align:center;'> Stochastic Gradient Boosting</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c333871c-1485-4886-b14e-f5c5e8521966",
   "metadata": {},
   "source": [
    "### Gradient Boosting Cons\n",
    "* GB involves exhaustive search procedure\n",
    "* Each CART is trained to find the best split points and features\n",
    "* may lead to CARTs using the same split points and maybe the same features\n",
    "\n",
    "* Stochastic Gradient Boosting\n",
    "    * each CART is trained on a random subset of the training data\n",
    "    *  the subset is sampled without replacement (40-80% of the training set)\n",
    "    * features are sampled without replacement when choosing split points\n",
    "    * Effect: further diversity in the ensemble of trees\n",
    "    * only a fraction of the training instances are passed to a tree\n",
    "        * the sampled data is then used to train the tree\n",
    "        * not all features are considered only a random sample\n",
    "        * once the tree is trained residual errors can be computed\n",
    "        * these residuals are multiplied by \"eta\" and are fed to the next tree in the ensemble\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0960930d-dcce-4e80-96c6-0cea4f06f9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.282236502853863"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = mpg.drop(['mpg','origin'], axis=1)\n",
    "y = mpg['mpg']\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "sgbt = GradientBoostingRegressor(max_depth=1, subsample=0.8, max_features=0.2, n_estimators=300, random_state=SEED)\n",
    "\n",
    "sgbt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgbt.predict(X_test)\n",
    "\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "rmse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e175ca-249d-481f-9e6b-393447c7f684",
   "metadata": {},
   "source": [
    "# <hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88059071-fb5a-461f-8430-dc8f594c08c3",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\"><font color='red'> Model Tuning</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e91a88-2a59-40cc-8c91-bcbbd9cbe7b7",
   "metadata": {},
   "source": [
    "# Tuning a carts hyperparameters\n",
    "* to obtain better performance, the hyperparameters should be tuned\n",
    "* parameters are learned from data through training\n",
    "    * split point of a node\n",
    "    * split feature of a node, etc\n",
    "* Hyperparameters\n",
    "    * not learned from data\n",
    "    * need to be adjusted (tuned) before implementing the model\n",
    "    * examples: max_depth, min_samples_leaf, etc\n",
    "* Tuning is searching for the optimal hyperparameters for a learning algorithm\n",
    "* solution : finding the set of hyperparameters that yield the optimal model\n",
    "* the optimal model yields an optimal score\n",
    "* score function measures the agreement to accuracy for classification and R^2 for regression\n",
    "* cross validation is used to estimate the generalization performance\n",
    "    * i.e. how well it will when you introduce new data to predict on\n",
    "* Why tune?\n",
    "    * the default settings are not optimal for ALL problems\n",
    "* approaches:\n",
    "    * Grid search\n",
    "    * random search\n",
    "    * bayesion optimization\n",
    "    * genetic algorithms\n",
    "* We focus on grid-search here\n",
    "    * GridCV\n",
    "        * manually set a grid of discrete hyperparameter values\n",
    "            * like in a list to pass to the pipeline\n",
    "        * set a metric for scoring model performance\n",
    "        * search exhaustively through the grid\n",
    "        * for each set of hyperparameters you evaluate each model's CV score\n",
    "        * the optimal hyperparameters are those of the model achieving the best CV score\n",
    "        * Grid search suffers from the curse of dimensionality\n",
    "            * the larger the grid, the longer it takes to find a solution\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "fd23a702-9da5-4c44-8dcc-0afc2dee4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "cancer = pd.read_csv('data/wisconsin_breast_cancer.csv')\n",
    "cancer.head()\n",
    "\n",
    "X = cancer.drop(['id','diagnosis', 'Unnamed: 32'], axis=1)\n",
    "y = cancer.diagnosis\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6f57ceca-36fc-4439-87d6-ea144ba400a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary with the hyperparameters you want to tune (this is the GRID)\n",
    "params_dt = {'max_depth': [3,4,5,6],\n",
    "            'min_samples_leaf':[0.04, 0.06, 0.08],\n",
    "            'max_features': [0.2, 0.4, 0.6, 0.8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "320ba1d1-104c-43a9-be1a-6186b26ee8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dt = GridSearchCV(estimator=dt,\n",
    "                      param_grid=params_dt,\n",
    "                      scoring='accuracy',\n",
    "                      cv=10,\n",
    "                      n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2d552bf4-dc92-4f43-baef-99552b228ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'max_features': 0.2, 'min_samples_leaf': 0.06}\n"
     ]
    }
   ],
   "source": [
    "#fit grid to training set\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "#extract best params from grid_dt\n",
    "best_params = grid_dt.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d340ec86-5fb0-423c-94c3-0751d2ecd2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9342995169082127\n"
     ]
    }
   ],
   "source": [
    "#extract best CV score from grid_dt\n",
    "best_CV_score = grid_dt.best_score_\n",
    "print(best_CV_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b4877e31-a892-44ba-98d5-7a44975533ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "#extract best model\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "#evaluate best_model's test set accuracy\n",
    "test_acc = best_model.score(X_test, y_test)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9438ec-4a32-4a12-84a0-e29a9959632f",
   "metadata": {},
   "source": [
    "# <hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28a14c-0da0-4309-a2c1-218fc85ec917",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\"> Tuning An RF's Hyperparameters<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852f220-1fc8-48c5-a1e0-dfe5271e0952",
   "metadata": {},
   "source": [
    "# Tuning random forests\n",
    "* ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8e95e764-63ce-4613-b87e-312125413e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 1\n",
    "\n",
    "rf = RandomForestRegressor(random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "97d02afa-9020-4e10-9655-4cc29b245da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "94303a30-0b2e-422c-9076-a2403d9f43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mpg.drop(['mpg','origin'], axis=1)\n",
    "y = mpg['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5cfe0acb-a036-459a-9974-fef8b2f32b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a984cf94-57da-4d52-9110-baf21d2d8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params_rf = {\"n_estimators\": [300, 400, 500],\n",
    "            'max_depth': [4, 6, 8],\n",
    "            'min_samples_leaf': [0.1, 0.2],\n",
    "            'max_features': ['log2', 'sqrt']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "21ed5aff-d51f-4852-b89e-7537c9d08ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf = GridSearchCV(estimator=rf,\n",
    "                      param_grid=params_rf,\n",
    "                      cv=3,\n",
    "                      scoring='neg_mean_squared_error',\n",
    "                      verbose=1,\n",
    "                      n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4b848d52-7641-4b08-a262-89b5f068afd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestRegressor(random_state=1), n_jobs=-1,\n",
       "             param_grid={'max_depth': [4, 6, 8],\n",
       "                         'max_features': ['log2', 'sqrt'],\n",
       "                         'min_samples_leaf': [0.1, 0.2],\n",
       "                         'n_estimators': [300, 400, 500]},\n",
       "             scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit grid_rf\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a25517f2-432f-4150-9b88-6d5e92fd5586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 4, 'max_features': 'log2', 'min_samples_leaf': 0.1, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "#extract best parameters\n",
    "best_hyperparameters = grid_rf.best_params_\n",
    "print(best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "35afc1bf-09cb-4180-89f5-426649e20755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.919344081331026\n"
     ]
    }
   ],
   "source": [
    "#extract best model from grid_rf\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "#predic the test labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "#evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "print(rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ae49f-d965-4907-8ffe-d16dbbe3e16d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
